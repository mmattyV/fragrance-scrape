{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fragrantica Perfume Data Science Project\n",
    "\n",
    "## Project Overview\n",
    "This project involves web scraping perfume data from the website Fragrantica. The goal is to analyze the data to uncover insights about the perfumes, such as the most popular notes, trends in fragrance releases, and user preferences.\n",
    "\n",
    "(Resumen del Proyecto\n",
    "Este proyecto implica la recolección de datos de perfumes mediante web scraping del sitio web Fragrantica. El objetivo es analizar los datos para descubrir información sobre los perfumes, como las notas más populares, tendencias en lanzamientos de fragancias y preferencias de los usuarios.)\n",
    "\n",
    "## Steps Involved\n",
    "1. **Data Collection**: Web scraping Fragrantica to collect perfume-related data.\n",
    "   \n",
    "   (Recolección de Datos: Recolección de datos relacionados con perfumes mediante web scraping de Fragrantica.)\n",
    "\n",
    "2. **Data Cleaning and Preprocessing**: Cleaning and structuring the data for analysis.\n",
    "   \n",
    "   (Limpieza y Preprocesamiento de Datos: Limpieza y estructuración de los datos para el análisis.)\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA)**: Exploring the data to identify patterns, trends, and insights.\n",
    "   \n",
    "   (Análisis Exploratorio de Datos (EDA): Exploración de los datos para identificar patrones, tendencias y conocimientos.)\n",
    "\n",
    "4. **Data Visualization**: Visualizing the findings through charts and graphs.\n",
    "   \n",
    "   (Visualización de Datos: Visualización de los hallazgos mediante gráficos y diagramas.)\n",
    "\n",
    "5. **Modeling and Predictions**: Building models to predict trends and preferences in perfumes.\n",
    "   \n",
    "   (Modelado y Predicciones: Creación de modelos para predecir tendencias y preferencias en perfumes.)\n",
    "\n",
    "6. **Conclusion and Insights**: Summarizing the findings and insights gained from the analysis.\n",
    "   \n",
    "   (Conclusiones y Conocimientos: Resumen de los hallazgos y conocimientos obtenidos del análisis.)\n",
    "\n",
    "## Tools and Libraries\n",
    "- **Python**: Programming language for data manipulation and analysis.\n",
    "  \n",
    "  (Python: Lenguaje de programación para la manipulación y análisis de datos.)\n",
    "\n",
    "- **Pandas**: Library for data manipulation and analysis.\n",
    "  \n",
    "  (Pandas: Biblioteca para la manipulación y análisis de datos.)\n",
    "\n",
    "- **BeautifulSoup/Scrapy**: Tools for web scraping.\n",
    "  \n",
    "  (BeautifulSoup/Scrapy: Herramientas para la recolección de datos mediante web scraping.)\n",
    "\n",
    "- **Matplotlib/Seaborn**: Libraries for data visualization.\n",
    "  \n",
    "  (Matplotlib/Seaborn: Bibliotecas para la visualización de datos.)\n",
    "\n",
    "## Data Source\n",
    "The data for this project was collected from Fragrantica.com using web scraping techniques.\n",
    "\n",
    "(Fuente de Datos\n",
    "Los datos para este proyecto fueron recolectados de Fragrantica.com utilizando técnicas de web scraping.)\n",
    "\n",
    "## Author\n",
    "Your Name (Tu Nombre)\n",
    "\n",
    "## Date\n",
    "August 2024 (Agosto 2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Process\n",
    "\n",
    "### Overview\n",
    "The data for this project was collected from Fragrantica.com using web scraping techniques. Web scraping is a method of extracting information from websites by mimicking human interaction with the site. This data is crucial for analyzing trends and patterns in perfumes.\n",
    "\n",
    "(Resumen\n",
    "Los datos para este proyecto fueron recolectados de Fragrantica.com utilizando técnicas de web scraping. El web scraping es un método de extracción de información de sitios web imitando la interacción humana con el sitio. Estos datos son cruciales para analizar tendencias y patrones en perfumes.)\n",
    "\n",
    "### Steps Involved\n",
    "\n",
    "1. **Identifying the Target URLs**:\n",
    "   - First, I identified the pages on Fragrantica.com that contained the information needed, such as perfume names, notes, brands, and user ratings.\n",
    "   \n",
    "   (Identificación de las URLs Objetivo:\n",
    "   - Primero, identifiqué las páginas en Fragrantica.com que contenían la información necesaria, como nombres de perfumes, notas, marcas y calificaciones de usuarios.)\n",
    "\n",
    "2. **Inspecting the Web Page Structure**:\n",
    "   - Using the browser's Developer Tools, I inspected the HTML structure of the web pages to identify the tags and classes containing the desired data.\n",
    "   \n",
    "   (Inspección de la Estructura de la Página Web:\n",
    "   - Usando las Herramientas de Desarrollador del navegador, inspeccioné la estructura HTML de las páginas web para identificar las etiquetas y clases que contenían los datos deseados.)\n",
    "\n",
    "3. **Writing the Scraping Script**:\n",
    "   - I wrote a Python script using libraries like `BeautifulSoup` and `requests` to send HTTP requests to the target URLs and parse the HTML content.\n",
    "   - The script extracted the relevant data and stored it in a structured format, such as a CSV file.\n",
    "   \n",
    "   (Escritura del Script de Scraping:\n",
    "   - Escribí un script en Python usando bibliotecas como `BeautifulSoup` y `requests` para enviar solicitudes HTTP a las URLs objetivo y analizar el contenido HTML.\n",
    "   - El script extrajo los datos relevantes y los guardó en un formato estructurado, como un archivo CSV.)\n",
    "\n",
    "4. **Handling Pagination and Dynamic Content**:\n",
    "   - For pages with multiple sections or pagination, I implemented logic to navigate through all pages and collect data from each one.\n",
    "   - In case of dynamic content loaded by JavaScript, I used tools like `Selenium` to simulate user interaction and capture the necessary data.\n",
    "   \n",
    "   (Manejo de Paginación y Contenido Dinámico:\n",
    "   - Para páginas con múltiples secciones o paginación, implementé lógica para navegar por todas las páginas y recopilar datos de cada una.\n",
    "   - En caso de contenido dinámico cargado por JavaScript, utilicé herramientas como `Selenium` para simular la interacción del usuario y capturar los datos necesarios.)\n",
    "\n",
    "5. **Storing the Data**:\n",
    "   - The scraped data was saved in CSV files for further analysis. Each CSV file corresponds to a different category or section of perfumes.\n",
    "   \n",
    "   (Almacenamiento de los Datos:\n",
    "   - Los datos recolectados se guardaron en archivos CSV para un análisis posterior. Cada archivo CSV corresponde a una categoría o sección diferente de perfumes.)\n",
    "\n",
    "6. **Ethical Considerations**:\n",
    "   - While scraping, I ensured that the website’s `robots.txt` file was respected and that the scraping was conducted in a manner that does not overload the website.\n",
    "   \n",
    "   (Consideraciones Éticas:\n",
    "   - Mientras hacía scraping, me aseguré de que se respetara el archivo `robots.txt` del sitio web y de que el scraping se realizara de manera que no sobrecargara el sitio web.)\n",
    "\n",
    "### Code Example\n",
    "Below is a code snippet that illustrates how the web scraping was performed using Python:\n",
    "\n",
    "(Código Ejemplo\n",
    "A continuación, un fragmento de código que ilustra cómo se realizó el scraping web usando Python:)\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Example URL (URL de ejemplo)\n",
    "url = 'https://www.fragrantica.com/perfume/Brand/Perfume-Name.html'\n",
    "\n",
    "# Sending a GET request to the website (Enviando una solicitud GET al sitio web)\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parsing the HTML content (Analizando el contenido HTML)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extracting data (Extracción de datos)\n",
    "perfume_name = soup.find('h1', class_='perfume-name').text.strip()\n",
    "brand_name = soup.find('h2', class_='brand-name').text.strip()\n",
    "notes = [note.text for note in soup.find_all('span', class_='note')]\n",
    "\n",
    "print(f'Perfume: {perfume_name}, Brand: {brand_name}, Notes: {notes}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Process\n",
    "\n",
    "### Overview\n",
    "Once the data was collected through web scraping, it required cleaning and preprocessing to ensure it was in a suitable format for analysis. Data cleaning is essential to remove inconsistencies, correct errors, and handle missing values, which could otherwise affect the quality and accuracy of the analysis.\n",
    "\n",
    "(Resumen\n",
    "Una vez recolectados los datos mediante web scraping, fue necesario limpiarlos y preprocesarlos para asegurar que estuvieran en un formato adecuado para el análisis. La limpieza de datos es esencial para eliminar inconsistencias, corregir errores y manejar valores faltantes, que de otro modo podrían afectar la calidad y precisión del análisis.)\n",
    "\n",
    "### Steps Involved\n",
    "\n",
    "1. **Handling Missing Values**:\n",
    "   - Identified any missing values in the dataset using `isnull()` and handled them by either filling them with appropriate values (e.g., mean, median) or removing the rows/columns if necessary.\n",
    "   \n",
    "   (Manejo de Valores Faltantes:\n",
    "   - Identifiqué los valores faltantes en el conjunto de datos usando `isnull()` y los manejé llenándolos con valores apropiados (por ejemplo, media, mediana) o eliminando las filas/columnas si era necesario.)\n",
    "\n",
    "2. **Removing Duplicates**:\n",
    "   - Checked for and removed duplicate entries to avoid redundant data using `drop_duplicates()`. This ensures that each record in the dataset is unique.\n",
    "   \n",
    "   (Eliminación de Duplicados:\n",
    "   - Verifiqué y eliminé entradas duplicadas para evitar datos redundantes usando `drop_duplicates()`. Esto asegura que cada registro en el conjunto de datos sea único.)\n",
    "\n",
    "3. **Correcting Data Types**:\n",
    "   - Ensured that each column had the correct data type (e.g., converting strings to dates, numbers to integers/floats) to facilitate accurate analysis and prevent errors during computations.\n",
    "   \n",
    "   (Corrección de Tipos de Datos:\n",
    "   - Aseguré que cada columna tuviera el tipo de dato correcto (por ejemplo, convirtiendo cadenas a fechas, números a enteros/decimales) para facilitar un análisis preciso y prevenir errores durante los cálculos.)\n",
    "\n",
    "4. **Standardizing Text Data**:\n",
    "   - Standardized text data by converting all text to lower case and stripping leading/trailing whitespaces to maintain consistency.\n",
    "   - Corrected common spelling errors or variations in the text (e.g., \"floral\" vs \"florals\").\n",
    "   \n",
    "   (Estandarización de Datos de Texto:\n",
    "   - Estandaricé los datos de texto convirtiendo todo a minúsculas y eliminando espacios en blanco al principio/final para mantener la consistencia.\n",
    "   - Corregí errores ortográficos comunes o variaciones en el texto (por ejemplo, \"floral\" vs \"florals\").)\n",
    "\n",
    "5. **Handling Outliers**:\n",
    "   - Identified and handled outliers in numerical data using statistical methods like Z-scores or IQR (Interquartile Range) to determine if they should be removed or adjusted.\n",
    "   \n",
    "   (Manejo de Valores Atípicos:\n",
    "   - Identifiqué y manejé valores atípicos en datos numéricos utilizando métodos estadísticos como puntajes Z o IQR (rango intercuartílico) para determinar si debían ser eliminados o ajustados.)\n",
    "\n",
    "6. **Parsing and Extracting Features**:\n",
    "   - Extracted additional features from existing columns where applicable, such as splitting a \"date\" column into \"year\", \"month\", and \"day\" columns.\n",
    "   \n",
    "   (Análisis y Extracción de Características:\n",
    "   - Extraje características adicionales de columnas existentes cuando fue aplicable, como dividir una columna \"fecha\" en columnas \"año\", \"mes\" y \"día\".)\n",
    "\n",
    "7. **Data Validation**:\n",
    "   - Validated the cleaned data to ensure there were no inconsistencies or errors introduced during the cleaning process. This included checking for correct data types, valid ranges for numerical data, and consistency in categorical data.\n",
    "   \n",
    "   (Validación de Datos:\n",
    "   - Validé los datos limpiados para asegurar que no hubiera inconsistencias o errores introducidos durante el proceso de limpieza. Esto incluyó verificar los tipos de datos correctos, rangos válidos para datos numéricos y consistencia en los datos categóricos.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish the task of concatenating  CSV files into a single file, I use the pandas library. The code will load each of the CSV files, concatenate them into a single DataFrame, and then save the result to a new CSV file named fra_scraping.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Leyendo los archivos CSV en un bucle, los leemos en un DataFrame y los agregamos a una lista.)\n",
    "\n",
    "(Concatenando los DataFrames usando pd.concat() en un solo DataFrame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file saved to: data/Fragrantica/fra_scraping.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing the CSV files (Definir la ruta a la carpeta que contiene los archivos CSV)\n",
    "folder_path = 'data/Fragrantica/'  \n",
    "# List of all CSV files to concatenate (Lista de todos los archivos CSV para concatenar)\n",
    "csv_files = [\n",
    "    'fra_perfumes1.csv',\n",
    "    'fra_perfumes2.csv',\n",
    "    'fra_perfumes3.csv',\n",
    "    'fra_perfumes4.csv',\n",
    "    'fra_perfumes5.csv',\n",
    "    'fra_perfumes6.csv',\n",
    "    'fra_perfumes7.csv',\n",
    "    'fra_perfumes8.csv'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to hold DataFrames (Inicializar una lista vacía para contener DataFrames)\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each file, read it into a DataFrame, and append to the list (Bucle a través de cada archivo, leerlo en un DataFrame, y agregar a la lista)\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame (Concatenar todos los DataFrames en un solo DataFrame)\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file (Guardar el DataFrame combinado en un nuevo archivo CSV)\n",
    "output_file_path = os.path.join(folder_path, 'fra_scraping.csv')\n",
    "combined_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Combined CSV file saved to: {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
